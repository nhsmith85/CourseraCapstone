---
output: html_document
---

## Data Acquisition

The first step in the process of building a word prediction application is using a set of text data from the language you want to build your model in - this case is English. The good folks at Johns Hopkins Bloomberg School of Public Health provided us with a dataset of the following size:

***
Type | File Size | Line Count 
----|----|-----
**News** | 249.6 MB | 1,010,242
**Blog** | 248.5 MB | 899,288
**Twitter**| 301.4 MB | 2,360,148
***

I randomly selected 50,000 rows of text data from each of the blog, news, and twitter datasets and combined them into a single dataset of 150,000 rows and roughly 13 MB. Below is the code used to select the random sample and to save that file into another directory so we can create a database and Corpus from it. 

``` {r, eval=FALSE}
# Sampling
set.seed(300)
Snews <- sample(news, size = 50000) # random sample of 50,000 lines
write(Snews,"ModelSample/en_US.news.sample.txt")
set.seed(322)
Sblogs <- sample(blogs, size = 50000)
write(Sblogs,"ModelSample/en_US.blogs.sample.txt")
set.seed(323)
Stwit <- sample(twit, size = 50000)
write(Stwit,"ModelSample/en_US.twitter.sample.txt")

Sample <- c(Snews, Sblogs, Stwit)

```

***

## Building the Corpus
In order to make use of all the great analysis tools in the `tm` package written by [Ingo Feinerer](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) we need to build a Corpus using the `tm` package to make a database to save our samples in. We will have to perform a lot of data cleansing on the data set to isolate what's important and make it easier to use the `tm` package to work it's magic. We will use a custom removal function that will take whatever general expression we want to remove and replace it with blank space (which we'll remove later all at once). We will remove URLs, dashes, and RT/via's from twitter file.

``` {r, eval=FALSE}
##############################
##### MAKING THE CORPUS ######
library(tm)
myCorpus <- Corpus(VectorSource(Sample))

# CUSTOM REMOVALS
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
# remove URLs
myCorpus <- tm_map(myCorpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
# removing / and @
myCorpus <- tm_map(myCorpus, toSpace,  "/|@|\\|")
# remove retweet and via labels
myCorpus <- tm_map(myCorpus, toSpace, "RT |via ")

# STANDARD REMOVALS  (leaving stopwords in)
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
```


If we don't want to accidently predict a curse word in our app then we have to make sure they've been taken out of our training data. We will source our list of curse words from this [site](http://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/).
``` {r, eval=FALSE}
# PROFANITY FILTERING
# setwd("/Users/nathansmith/CourseraCapstone/en/final/en_US")
url <- "http://www.frontgatemedia.com/new/wp-content/uploads/2014/03/Terms-to-Block.csv"
if (!file.exists("profanity.csv")) {
    download.file(url, destfile = "profanity.csv", method = "curl")
}
profanity <- read.csv("profanity.csv", stringsAsFactors=FALSE, skip = 3)
profanity <- profanity[,2]
profanity <- unlist(gsub(",", "", profanity))
profanity <- unique(profanity)
myCorpus <- tm_map(myCorpus, removeWords, profanity)
rm(profanity,Sample,Sblogs,Snews,Stwit)
```

The final step in data cleansing for now is to remove all the extra white space we've created in our previous steps. 
``` {r, eval=FALSE}
# WHITESPACE REMOVAL
myCorpus <- tm_map(myCorpus, stripWhitespace)
```

#### Tokenization
We need to understand the frequencies of words and word pairs across the three data sources. Additionally, we need know to the frequencies of what are called n-grams (1-gram is one word, bi-gram is a word pair, and tri-gram is a set of three words). We will use the `RWeka` and `tm` packages for this. 

``` {r, eval = FALSE}
library(tm)
library(RWeka)

# tokenizers
options(mc.cores=1)
UniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
BiGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TriGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
QuadGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
```

The next tool we use is called a Document Term Matrix which is essentially a table that displays how many times a word appears using 1 or 0 in each line. We will use this tool to get our count of n-grams to determine which ones are most common in each data set and display them in a wordcloud and bar chart which you can see on the "Explore" tab.

``` {r, eval=FALSE}
# make DTMs
uni.DTM <- DocumentTermMatrix(myCorpus, control = list(tokenize = UniGramTokenizer))
bi.DTM <- DocumentTermMatrix(myCorpus, control = list(tokenize = BiGramTokenizer))
tri.DTM <- DocumentTermMatrix(myCorpus, control = list(tokenize = TriGramTokenizer))
quad.DTM <- DocumentTermMatrix(myCorpus, control = list(tokenize = QuadGramTokenizer))
rm(myCorpus)
```

The DTM will pick up a lot of infrequently used words, so we need to reduce it's size and get rid of the rarely used words and phrases. 
``` {r, eval = FALSE}
# remove sparse terms
uni.sparse <- removeSparseTerms(uni.DTM, 0.999)
bi.sparse <- removeSparseTerms(bi.DTM, 0.9997) # extending the possible list
tri.sparse <- removeSparseTerms(tri.DTM, 0.9999) # extending the possible list
quad.sparse <- removeSparseTerms(quad.DTM, 0.9999) # extending the possible list
rm(uni.DTM,bi.DTM,tri.DTM, quad.DTM)
```

Using the sparse DTM we can then find word frequencies which will use to rank the most likely words or phrases. 
``` {r, eval=FALSE}
# find frequency rankings
uni.freq <- sort(colSums(as.matrix(uni.sparse)), decreasing=TRUE)
bi.freq <- sort(colSums(as.matrix(bi.sparse)), decreasing=TRUE)
tri.freq <- sort(colSums(as.matrix(tri.sparse)), decreasing=TRUE)
quad.freq <- sort(colSums(as.matrix(quad.sparse)), decreasing=TRUE)
```

We need to make these tables into dataframes to perform operations on them. 
``` {r, eval=FALSE}
# making data frames of frequencies
df.uni <- data.frame(word = names(uni.freq), freq = unname(uni.freq),  stringsAsFactors = FALSE)
df.bi <- data.frame(word = names(bi.freq), freq = unname(bi.freq),  stringsAsFactors = FALSE)
df.tri <- data.frame(word = names(tri.freq), freq = unname(tri.freq),  stringsAsFactors = FALSE)
df.quad <- data.frame(word = names(quad.freq), freq = unname(quad.freq),  stringsAsFactors = FALSE)
# setwd("/Users/nathansmith/CourseraCapstone/shiny")
save(df.uni,df.bi,df.tri,df.quad, file = "freq.RData")
```

The final task is to separate out all the phrases with the first words in the phrase, the final word, and it's ranking. 
``` {r, eval=FALSE}

# split tokened n grams and unlist so you can search for a match on the first three letters and return the final one
library(qdap)

# 4-gram data frame assembly
df4 <- colsplit2df(data.frame(names(quad.freq)), sep = " ")
df4$minus <- paste( df4[,1], df4[,2], df4[,3])
pred4 <- data.frame( match=df4$minus, pred=df4$X4, freq=unname(quad.freq), stringsAsFactors = FALSE)


# 3-gram data frame assembly
df3 <- colsplit2df(data.frame(names(tri.freq)), sep = " ")
df3$minus <- paste( df3[,1], df3[,2])
pred3 <- data.frame( match=df3$minus, pred=df3$X3, freq=unname(tri.freq), stringsAsFactors = FALSE)
str(pred3)
head(pred3)
pred3[pred3$match == c("a lot"),]

# 2-gram data frame assembly
df2 <- colsplit2df(data.frame(names(bi.freq)), sep = " ")
df2$minus <- paste( df2[,1] )
pred2 <- data.frame( match=df2$minus, pred=df2$X2, freq=unname(bi.freq), stringsAsFactors = FALSE)
str(pred2)
head(pred2)
pred2[pred2$match == c("often"),]

# 1-gram data frame assembly
pred1 <- data.frame(match=names(uni.freq), pred=names(uni.freq), freq=unname(uni.freq), stringsAsFactors = FALSE)
head(pred1)

save(pred4,pred3,pred2,pred1, file = "pred.RData")
```









